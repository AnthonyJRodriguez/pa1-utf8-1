//The 3-byte sequence 11100000 10000000 10100001 in UTF-8 actually represents the character `!`, which has the Unicode code point U+0021. Normally, `!` would just need one byte in UTF-8 (00100001), but here, it's using three bytes—this is what we call an verlong encoding. Overlong encodings use more bytes than necessary to represent a character, and for `!`, there are a few other ways to encode it: it could be done with two bytes (11000000 10100001) or even four bytes (11110000 10000000 10000000 10100001). These alternate ways don’t add any real value since they just waste space.

//Another interesting example is the DELETE character, U+007F. This character has three possible encodings in UTF-8: a 1-byte encoding (01111111), a 2-byte overlong encoding (11000001 10111111), and a 3-byte overlong encoding (11100000 10000001 10111111). Unlike `!`, though, it doesn’t have a valid 4-byte encoding, so it’s limited to three ways of encoding.

//These multiple encodings, especially for common ASCII characters, can actually cause problems. They introduce security issues; for instance, certain characters like `NUL`, which can terminate strings in some programming languages, could be masked by an overlong encoding, letting potentially harmful data slip through filters unnoticed. They also make data consistency harder to maintain—two identical-looking strings might not match if they use different encodings. Plus, overlong encodings take up more space than needed, adding unnecessary processing and storage overhead. To keep things efficient and secure, Unicode prohibits these overlong encodings in UTF-8 today.